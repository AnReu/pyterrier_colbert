{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTerrier 0.3.1 has loaded Terrier 5.4 (built by craigm on 2021-01-16 14:17)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import pyterrier as pt\n",
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"/users/tr.craigm/projects/pyterrier/pyterrier_bert/ColBERT/GOOD_MODELS/colbert.dnn\"\n",
    "index_root=\"/tmp/vaswani_colbert\"\n",
    "index_name=\"tesdting\"\n",
    "\n",
    "dataset = pt.get_dataset(\"irds:vaswani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'pyterrier_colbert.indexing' from '/users/tr.craigm/projects/pyterrier/pyterrier_colbert/pyterrier_colbert/indexing.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import pyterrier_colbert.indexing\n",
    "from importlib import reload\n",
    "reload(pyterrier_colbert.indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO what is chunksize\n",
    "indexer = pyterrier_colbert.indexing.ColBERTIndexer(checkpoint, index_root, index_name, chunksize=3)\n",
    "if os.path.exists(indexer.args.index_path):\n",
    "    shutil.rmtree(indexer.args.index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Feb 21, 14:26:10] [0] \t\t #> Local args.bsize = 128\n",
      "[Feb 21, 14:26:10] [0] \t\t #> args.index_root = /tmp/vaswani_colbert\n",
      "[Feb 21, 14:26:10] [0] \t\t #> self.possible_subset_sizes = [69905]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Feb 21, 14:26:18] #> Loading model checkpoint.\n",
      "[Feb 21, 14:26:18] #> Loading checkpoint /users/tr.craigm/projects/pyterrier/pyterrier_bert/ColBERT/GOOD_MODELS/colbert.dnn\n",
      "[Feb 21, 14:26:19] #> checkpoint['epoch'] = 0\n",
      "[Feb 21, 14:26:19] #> checkpoint['batch'] = 44500\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Feb 21, 14:26:20] #> Note: Output directory /tmp/vaswani_colbert already exists\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Feb 21, 14:26:20] #> Creating directory /tmp/vaswani_colbert/tesdting \n",
      "\n",
      "\n",
      "encoding:   0%|          | 0/11429 [00:00<?, ?d/s]\n",
      "encoding:  64%|██████▍   | 7340/11429 [00:00<00:00, 73393.42d/s]\n",
      "vaswani documents: 100%|██████████| 11429/11429 [00:00<00:00, 77589.45it/s]\n",
      "encoding: 100%|██████████| 11429/11429 [00:00<00:00, 73575.25d/s]docFromText on 11429 documents\n",
      "\n",
      "tokens doc 0: 180\n",
      "total tokens 2057220\n",
      "11429/tmp/vaswani_colbert/tesdting/0.pt\n",
      "\n",
      "[Feb 21, 14:27:19] [0] \t\t #> Completed batch #0 (starting at passage #0) \t\tPassages/min: 11.6k (overall),  11.7k (this encoding),  330.6M (this saving)\n",
      "[Feb 21, 14:27:19] [0] \t\t [NOTE] Done with local share.\n",
      "[Feb 21, 14:27:19] [0] \t\t #> Joining saver thread.\n",
      "[Feb 21, 14:27:20] [0] \t\t #> Saved batch #0 to /tmp/vaswani_colbert/tesdting/0.pt \t\t Saving Throughput = 1.8M passages per minute.\n",
      "\n",
      "#> num_embeddings = 581496\n",
      "[Feb 21, 14:27:20] #> Starting..\n",
      "[Feb 21, 14:27:20] #> Processing slice #1 of 1 (range 0..1).\n",
      "[Feb 21, 14:27:20] #> Will write to /tmp/vaswani_colbert/tesdting/ivfpq.100.faiss.\n",
      "[Feb 21, 14:27:20] #> Loading /tmp/vaswani_colbert/tesdting/0.sample ...\n",
      "#> Sample has shape (29074, 128)\n",
      "[Feb 21, 14:27:20] Preparing resources for 3 GPUs.\n",
      "[Feb 21, 14:27:20] #> Training with the vectors...\n",
      "[Feb 21, 14:27:20] #> Training now (using 3 GPUs)...\n",
      "2.073387384414673\n",
      "1.7172908782958984\n",
      "0.0006766319274902344\n",
      "[Feb 21, 14:27:23] Done training!\n",
      "\n",
      "[Feb 21, 14:27:23] #> Indexing the vectors...\n",
      "[Feb 21, 14:27:23] #> Loading ('/tmp/vaswani_colbert/tesdting/0.pt', None, None) (from queue)...\n",
      "[Feb 21, 14:27:24] #> Processing a sub_collection with shape (581496, 128)\n",
      "[Feb 21, 14:27:24] Add data with shape (581496, 128) (offset = 0)..\n",
      "524288/581496 (0.387 s)   Flush indexes to CPU\n",
      "add(.) time: 0.603 s \t\t--\t\t index.ntotal = 581496\n",
      "[Feb 21, 14:27:25] Done indexing!\n",
      "[Feb 21, 14:27:25] Writing index to /tmp/vaswani_colbert/tesdting/ivfpq.100.faiss ...\n",
      "[Feb 21, 14:27:25] \n",
      "\n",
      "Done! All complete (for slice #1 of 1)!\n",
      "#> Faiss encoding complete\n",
      "#> Indexing complete, 74.943794\n"
     ]
    }
   ],
   "source": [
    "indexer.index(dataset.get_corpus_iter(), num_docs=11_429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytcolbert = indexer.ranking_factory(memtype=\"mem\")\n",
    "\n",
    "#this is equivalent\n",
    "#pytcolbert = pyterrier_colbert.ranking.ColBERTFactory(checkpoint, index_root, index_name, memtype=\"mem\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 14:27:53] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:53] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:53] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:53] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:53] #> Done with embedding_ids_to_pids().\n",
      "qid 64 retrieved docs 5864\n",
      "64q [00:23,  3.19q/s][Feb 21, 14:27:54] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:54] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:54] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:54] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:54] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:54] #> Done with embedding_ids_to_pids().\n",
      "qid 65 retrieved docs 5053\n",
      "65q [00:23,  3.23q/s][Feb 21, 14:27:54] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:54] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:54] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:54] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:54] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:54] #> Done with embedding_ids_to_pids().\n",
      "qid 66 retrieved docs 5125\n",
      "66q [00:24,  3.27q/s][Feb 21, 14:27:54] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:54] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:54] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:54] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:54] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:54] #> Done with embedding_ids_to_pids().\n",
      "qid 67 retrieved docs 5538\n",
      "67q [00:24,  3.24q/s][Feb 21, 14:27:55] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:55] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:55] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:55] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:55] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:55] #> Done with embedding_ids_to_pids().\n",
      "qid 68 retrieved docs 5373\n",
      "68q [00:24,  3.11q/s][Feb 21, 14:27:55] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:55] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:55] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:55] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:55] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:55] #> Done with embedding_ids_to_pids().\n",
      "qid 69 retrieved docs 5175\n",
      "69q [00:25,  3.00q/s][Feb 21, 14:27:55] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:55] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:55] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:55] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:55] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:55] #> Done with embedding_ids_to_pids().\n",
      "qid 70 retrieved docs 6378\n",
      "70q [00:25,  2.69q/s][Feb 21, 14:27:56] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:56] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:56] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:56] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:56] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:56] #> Done with embedding_ids_to_pids().\n",
      "qid 71 retrieved docs 4831\n",
      "71q [00:26,  2.72q/s][Feb 21, 14:27:56] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:56] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:56] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:56] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:56] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:56] #> Done with embedding_ids_to_pids().\n",
      "qid 72 retrieved docs 4495\n",
      "72q [00:26,  2.79q/s][Feb 21, 14:27:57] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:57] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:57] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:57] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:57] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:57] #> Done with embedding_ids_to_pids().\n",
      "qid 73 retrieved docs 4014\n",
      "73q [00:26,  2.92q/s][Feb 21, 14:27:57] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:57] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:57] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:57] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:57] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:57] #> Done with embedding_ids_to_pids().\n",
      "qid 74 retrieved docs 3945\n",
      "74q [00:26,  3.06q/s][Feb 21, 14:27:57] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:57] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:57] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:57] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:57] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:57] #> Done with embedding_ids_to_pids().\n",
      "qid 75 retrieved docs 3843\n",
      "75q [00:27,  3.16q/s][Feb 21, 14:27:57] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:57] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:57] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:57] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:57] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:57] #> Done with embedding_ids_to_pids().\n",
      "qid 76 retrieved docs 3805\n",
      "76q [00:27,  3.27q/s][Feb 21, 14:27:58] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:58] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:58] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:58] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:58] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:58] #> Done with embedding_ids_to_pids().\n",
      "qid 77 retrieved docs 8270\n",
      "77q [00:28,  2.65q/s][Feb 21, 14:27:58] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:58] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:58] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:58] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:58] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:27:58] #> Done with embedding_ids_to_pids().\n",
      "qid 78 retrieved docs 6692\n",
      "78q [00:29,  1.60q/s][Feb 21, 14:27:59] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:27:59] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:27:59] #> Lookup the PIDs..\n",
      "[Feb 21, 14:27:59] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:27:59] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:00] #> Done with embedding_ids_to_pids().\n",
      "qid 79 retrieved docs 5713\n",
      "79q [00:29,  1.86q/s][Feb 21, 14:28:00] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:00] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:00] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:00] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:00] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:00] #> Done with embedding_ids_to_pids().\n",
      "qid 80 retrieved docs 6828\n",
      "80q [00:30,  2.01q/s][Feb 21, 14:28:00] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:00] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:00] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:00] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:00] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:00] #> Done with embedding_ids_to_pids().\n",
      "qid 81 retrieved docs 7972\n",
      "81q [00:30,  2.02q/s][Feb 21, 14:28:01] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:01] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:01] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:01] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:01] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:01] #> Done with embedding_ids_to_pids().\n",
      "qid 82 retrieved docs 5253\n",
      "82q [00:30,  2.22q/s][Feb 21, 14:28:01] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:01] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:01] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:01] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:01] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:01] #> Done with embedding_ids_to_pids().\n",
      "qid 83 retrieved docs 4423\n",
      "83q [00:31,  2.47q/s][Feb 21, 14:28:01] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:01] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:01] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:01] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:01] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:01] #> Done with embedding_ids_to_pids().\n",
      "qid 84 retrieved docs 5043\n",
      "84q [00:31,  2.56q/s][Feb 21, 14:28:02] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:02] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:02] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:02] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:02] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:02] #> Done with embedding_ids_to_pids().\n",
      "qid 85 retrieved docs 5553\n",
      "85q [00:31,  2.45q/s][Feb 21, 14:28:02] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:02] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:02] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:02] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:02] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:02] #> Done with embedding_ids_to_pids().\n",
      "qid 86 retrieved docs 4583\n",
      "86q [00:32,  2.54q/s][Feb 21, 14:28:03] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:03] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:03] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:03] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:03] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:03] #> Done with embedding_ids_to_pids().\n",
      "qid 87 retrieved docs 7708\n",
      "87q [00:32,  2.35q/s][Feb 21, 14:28:03] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:03] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:03] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:03] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:03] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:03] #> Done with embedding_ids_to_pids().\n",
      "qid 88 retrieved docs 7086\n",
      "88q [00:33,  2.08q/s][Feb 21, 14:28:04] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:04] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:04] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:04] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:04] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:04] #> Done with embedding_ids_to_pids().\n",
      "qid 89 retrieved docs 5990\n",
      "89q [00:33,  2.14q/s][Feb 21, 14:28:04] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:04] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:04] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:04] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:04] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:04] #> Done with embedding_ids_to_pids().\n",
      "qid 90 retrieved docs 6026\n",
      "90q [00:34,  2.12q/s][Feb 21, 14:28:05] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:05] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:05] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:05] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:05] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:05] #> Done with embedding_ids_to_pids().\n",
      "qid 91 retrieved docs 4719\n",
      "91q [00:34,  2.14q/s][Feb 21, 14:28:05] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:05] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:05] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:05] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:05] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:05] #> Done with embedding_ids_to_pids().\n",
      "qid 92 retrieved docs 5277\n",
      "92q [00:35,  2.17q/s][Feb 21, 14:28:05] #> Search in batches with faiss. \t\t Q.size() = torch.Size([1, 32, 128]), Q_faiss.size() = torch.Size([32, 128])\n",
      "[Feb 21, 14:28:05] #> Searching from 0 to 32...\n",
      "[Feb 21, 14:28:05] #> Lookup the PIDs..\n",
      "[Feb 21, 14:28:05] #> Converting to a list [shape = torch.Size([1, 32000])]..\n",
      "[Feb 21, 14:28:05] #> Removing duplicates (in parallel if large enough)..\n",
      "[Feb 21, 14:28:05] #> Done with embedding_ids_to_pids().\n",
      "qid 93 retrieved docs 5855\n",
      "93q [00:35,  2.61q/s]\n",
      "lookups: 100%|██████████| 6143/6143 [00:00<00:00, 21031.11d/s]\n",
      "lookups: 100%|██████████| 6839/6839 [00:00<00:00, 19461.33d/s]\n",
      "lookups: 100%|██████████| 4144/4144 [00:00<00:00, 21741.39d/s]\n",
      "lookups: 100%|██████████| 5589/5589 [00:00<00:00, 21435.50d/s]\n",
      "lookups: 100%|██████████| 5202/5202 [00:00<00:00, 15736.99d/s]\n",
      "lookups: 100%|██████████| 6831/6831 [00:00<00:00, 14292.85d/s]\n",
      "lookups: 100%|██████████| 5063/5063 [00:00<00:00, 15184.21d/s]\n",
      "lookups: 100%|██████████| 6555/6555 [00:00<00:00, 16714.50d/s]\n",
      "lookups: 100%|██████████| 6026/6026 [00:00<00:00, 18472.69d/s]\n",
      "lookups: 100%|██████████| 5072/5072 [00:00<00:00, 17752.93d/s]\n",
      "lookups: 100%|██████████| 5297/5297 [00:00<00:00, 17822.37d/s]\n",
      "lookups: 100%|██████████| 5809/5809 [00:00<00:00, 19519.74d/s]\n",
      "lookups: 100%|██████████| 5323/5323 [00:00<00:00, 17589.79d/s]\n",
      "lookups: 100%|██████████| 4351/4351 [00:00<00:00, 16789.32d/s]\n",
      "lookups: 100%|██████████| 5150/5150 [00:00<00:00, 18155.95d/s]\n",
      "lookups: 100%|██████████| 5256/5256 [00:00<00:00, 20251.93d/s]\n",
      "lookups: 100%|██████████| 4210/4210 [00:00<00:00, 17304.10d/s]\n",
      "lookups: 100%|██████████| 5405/5405 [00:00<00:00, 20283.79d/s]\n",
      "lookups: 100%|██████████| 4019/4019 [00:00<00:00, 15819.11d/s]\n",
      "lookups: 100%|██████████| 5690/5690 [00:00<00:00, 13170.86d/s]\n",
      "lookups: 100%|██████████| 5955/5955 [00:00<00:00, 13978.87d/s]\n",
      "lookups: 100%|██████████| 4309/4309 [00:00<00:00, 18940.42d/s]\n",
      "lookups: 100%|██████████| 6214/6214 [00:00<00:00, 16494.71d/s]\n",
      "lookups: 100%|██████████| 5890/5890 [00:00<00:00, 18928.34d/s]\n",
      "lookups: 100%|██████████| 6399/6399 [00:00<00:00, 14922.45d/s]\n",
      "lookups: 100%|██████████| 4513/4513 [00:00<00:00, 20038.08d/s]\n",
      "lookups: 100%|██████████| 5609/5609 [00:00<00:00, 15260.01d/s]\n",
      "lookups: 100%|██████████| 7053/7053 [00:00<00:00, 20159.74d/s]\n",
      "lookups: 100%|██████████| 4293/4293 [00:00<00:00, 16771.15d/s]\n",
      "lookups: 100%|██████████| 4834/4834 [00:00<00:00, 15138.87d/s]\n",
      "lookups: 100%|██████████| 5144/5144 [00:00<00:00, 14450.63d/s]\n",
      "lookups: 100%|██████████| 6089/6089 [00:00<00:00, 12866.35d/s]\n",
      "lookups: 100%|██████████| 4782/4782 [00:00<00:00, 12619.99d/s]\n",
      "lookups: 100%|██████████| 4448/4448 [00:00<00:00, 16098.20d/s]\n",
      "lookups: 100%|██████████| 4786/4786 [00:00<00:00, 16131.28d/s]\n",
      "lookups: 100%|██████████| 5036/5036 [00:00<00:00, 18875.92d/s]\n",
      "lookups: 100%|██████████| 3855/3855 [00:00<00:00, 12181.50d/s]\n",
      "lookups: 100%|██████████| 5729/5729 [00:00<00:00, 11992.76d/s]\n",
      "lookups: 100%|██████████| 4087/4087 [00:00<00:00, 16364.11d/s]\n",
      "lookups: 100%|██████████| 6323/6323 [00:00<00:00, 15183.75d/s]\n",
      "lookups: 100%|██████████| 5015/5015 [00:00<00:00, 18405.74d/s]\n",
      "lookups: 100%|██████████| 4614/4614 [00:00<00:00, 18296.05d/s]\n",
      "lookups: 100%|██████████| 4511/4511 [00:00<00:00, 17891.93d/s]\n",
      "lookups: 100%|██████████| 4964/4964 [00:00<00:00, 18423.43d/s]\n",
      "lookups: 100%|██████████| 4476/4476 [00:00<00:00, 12608.29d/s]\n",
      "lookups: 100%|██████████| 5192/5192 [00:00<00:00, 13712.55d/s]\n",
      "lookups: 100%|██████████| 7093/7093 [00:00<00:00, 15767.12d/s]\n",
      "lookups: 100%|██████████| 6656/6656 [00:00<00:00, 16020.84d/s]\n",
      "lookups: 100%|██████████| 6817/6817 [00:00<00:00, 9977.45d/s]\n",
      "lookups: 100%|██████████| 7203/7203 [00:00<00:00, 12286.21d/s]\n",
      "lookups: 100%|██████████| 6384/6384 [00:00<00:00, 21571.51d/s]\n",
      "lookups: 100%|██████████| 5889/5889 [00:00<00:00, 12389.68d/s]\n",
      "lookups: 100%|██████████| 5110/5110 [00:00<00:00, 10603.72d/s]\n",
      "lookups: 100%|██████████| 5555/5555 [00:00<00:00, 11353.81d/s]\n",
      "lookups: 100%|██████████| 5952/5952 [00:00<00:00, 12382.69d/s]\n",
      "lookups: 100%|██████████| 3631/3631 [00:00<00:00, 19320.13d/s]\n",
      "lookups: 100%|██████████| 5001/5001 [00:00<00:00, 15502.13d/s]\n",
      "lookups: 100%|██████████| 4334/4334 [00:00<00:00, 17521.78d/s]\n",
      "lookups: 100%|██████████| 3216/3216 [00:00<00:00, 18142.70d/s]\n",
      "lookups: 100%|██████████| 3586/3586 [00:00<00:00, 18272.04d/s]\n",
      "lookups: 100%|██████████| 5864/5864 [00:00<00:00, 17589.75d/s]\n",
      "lookups: 100%|██████████| 5053/5053 [00:00<00:00, 14808.84d/s]\n",
      "lookups: 100%|██████████| 5125/5125 [00:00<00:00, 14123.23d/s]\n",
      "lookups: 100%|██████████| 5538/5538 [00:00<00:00, 15868.28d/s]\n",
      "lookups: 100%|██████████| 5373/5373 [00:00<00:00, 18342.51d/s]\n",
      "lookups: 100%|██████████| 5175/5175 [00:00<00:00, 15803.81d/s]\n",
      "lookups: 100%|██████████| 5462/5462 [00:00<00:00, 17600.73d/s]\n",
      "lookups: 100%|██████████| 6378/6378 [00:00<00:00, 13104.95d/s]\n",
      "lookups: 100%|██████████| 4831/4831 [00:00<00:00, 15440.83d/s]\n",
      "lookups: 100%|██████████| 4495/4495 [00:00<00:00, 15911.66d/s]\n",
      "lookups: 100%|██████████| 4014/4014 [00:00<00:00, 15816.02d/s]\n",
      "lookups: 100%|██████████| 3945/3945 [00:00<00:00, 15918.40d/s]\n",
      "lookups: 100%|██████████| 3843/3843 [00:00<00:00, 17992.36d/s]\n",
      "lookups: 100%|██████████| 3805/3805 [00:00<00:00, 16599.45d/s]\n",
      "lookups: 100%|██████████| 8270/8270 [00:00<00:00, 21326.37d/s]\n",
      "lookups: 100%|██████████| 6692/6692 [00:00<00:00, 19696.68d/s]\n",
      "lookups: 100%|██████████| 5713/5713 [00:00<00:00, 18130.25d/s]\n",
      "lookups: 100%|██████████| 5137/5137 [00:00<00:00, 19838.83d/s]\n",
      "lookups: 100%|██████████| 6828/6828 [00:00<00:00, 16071.71d/s]\n",
      "lookups: 100%|██████████| 7972/7972 [00:00<00:00, 18408.47d/s]\n",
      "lookups: 100%|██████████| 5253/5253 [00:00<00:00, 19385.82d/s]\n",
      "lookups: 100%|██████████| 4423/4423 [00:00<00:00, 19419.92d/s]\n",
      "lookups: 100%|██████████| 5043/5043 [00:00<00:00, 17666.42d/s]\n",
      "lookups: 100%|██████████| 5553/5553 [00:00<00:00, 15876.88d/s]\n",
      "lookups: 100%|██████████| 4583/4583 [00:00<00:00, 14617.84d/s]\n",
      "lookups: 100%|██████████| 7708/7708 [00:00<00:00, 12458.72d/s]\n",
      "lookups: 100%|██████████| 7086/7086 [00:00<00:00, 15259.88d/s]\n",
      "lookups: 100%|██████████| 5990/5990 [00:00<00:00, 15664.10d/s]\n",
      "lookups: 100%|██████████| 5641/5641 [00:00<00:00, 15892.96d/s]\n",
      "lookups: 100%|██████████| 6026/6026 [00:00<00:00, 15682.01d/s]\n",
      "lookups: 100%|██████████| 4719/4719 [00:00<00:00, 13941.40d/s]\n",
      "lookups: 100%|██████████| 5277/5277 [00:00<00:00, 14425.11d/s]\n",
      "lookups: 100%|██████████| 5855/5855 [00:00<00:00, 15668.36d/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          name       map  map +  map -  map p-value\n",
       "0         BM25  0.296517    NaN    NaN          NaN\n",
       "1  ColBERT E2E  0.278845   49.0   44.0     0.231719"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>map</th>\n      <th>map +</th>\n      <th>map -</th>\n      <th>map p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BM25</td>\n      <td>0.296517</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ColBERT E2E</td>\n      <td>0.278845</td>\n      <td>49.0</td>\n      <td>44.0</td>\n      <td>0.231719</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset = pt.get_dataset(\"vaswani\")\n",
    "bm25 = pt.BatchRetrieve(dataset.get_index(), wmodel=\"BM25\")\n",
    "colbert_e2e = pytcolbert.end_to_end()\n",
    "pt.Experiment(\n",
    "    [bm25, colbert_e2e],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    eval_metrics=[\"map\"],\n",
    "    baseline=0,\n",
    "    names = [\"BM25\", \"ColBERT E2E\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}